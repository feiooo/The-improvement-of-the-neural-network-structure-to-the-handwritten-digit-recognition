# The-improvement-of-the-neural-network-structure-to-the-handwritten-digit-recognition
This project is class project. See project report for details.

## Coding Enviroment
Python 3.xx
Pytorch 1.7.0
torchvision 0.8.1
(or you can run the code directly on colab.)

## Project Result
| Method  | Correct/Total | Accuracy Total | Time Spent |
| --- | --- | --- | --- |
| Neural Network (CNN) | 9702/10000 | 97.02% | 1.6770s |

![image](https://github.com/feiooo/The-improvement-of-the-neural-network-structure-to-the-handwritten-digit-recognition/blob/main/img/test_output.jpg)


## References
[1] [CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/convolutional-networks/)
[2] LeCun, Yann, et al. "Gradient-based learning applied to document recognition." Proceedings of the IEEE 86.11 (1998): 2278-2324.
[3] [6 basic things to know about Convolution](https://medium.com/@bdhuma/6-basic-things-to-know-about-convolution-daef5e1bc411)
[4] [Max-pooling / Pooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling)
[5] [Batch Normalization and Dropout in Neural Networks with Pytorch](https://towardsdatascience.com/batch-normalization-and-dropout-in-neural-networks-explained-with-pytorch-47d7a8459bcd)
[6] [ReLU function](https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization7fef3a4cecec)
[7] [What is the Softmax Function?](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)
[8] Ruder, Sebastian. "An overview of gradient descent optimization algorithms." arXiv preprintarXiv:1609.04747(2016).
[9] [Loss Functions (Cross-Entropy Loss)](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)
